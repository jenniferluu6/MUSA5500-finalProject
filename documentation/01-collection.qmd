---
title: "Data Collection & Preparation"
---

## Overview

This section documents how flight diversion data was collected, processed, and prepared for analysis. Understanding the data pipeline is crucial for interpreting the results and understanding the limitations of this analysis.

---

## Data Sources

### Primary Source: BTS OTMC-OTP Database

**Source**: U.S. Bureau of Transportation Statistics
**Portal**: https://www.transtats.bts.gov/
**Database**: OTMC-OTP (Operations Time Series - On-Time Performance)

The BTS provides monthly CSV files containing detailed flight operation statistics for all U.S. carriers operating passenger services. Each file includes:

- Flight identifiers (carrier, flight number, date)
- Origin and destination airports
- Scheduled, actual, and diverted times
- Delay information
- Diversion airport information

**Coverage**: July 2021 - December 2024 (42 months)
**Records**: 25,386,632 total flight records
**Diversions Identified**: 64,816 diverted flights

### Secondary Source: Airport Coordinates

**Source**: Federal Aviation Administration & OpenFlights Database
**Portal**: https://www.faa.gov/ and https://openflights.org/
**Data**: Airport geographic coordinates (latitude/longitude)

Geographic data for U.S. airports was obtained to enable spatial analysis and visualization. Each airport is represented by its IATA code (3-letter code) and geographic coordinates.

**Coverage**: 408 airports across the United States
**Accuracy**: Latitude/longitude to seconds precision
---

## Data Download Process

### Steps to Reproduce Data Collection

The BTS data can be downloaded manually or via programmatic access:

#### Manual Download (Current Process)
1. Visit https://www.transtats.bts.gov/
2. Navigate to the "Downloads" section
3. Select "OTMC-OTP" database
4. Download monthly CSV files
5. Store in a `/data` directory

#### Programmatic Approach (Future Enhancement)
```python
# Example: Automating BTS data download
import requests
import pandas as pd

# The BTS API documentation would go here
# This is a template for how to automate collection
```

---

## Data Structure

### Key Variables in Raw Data

| Column | Description | Type | Notes |
|--------|-------------|------|-------|
| `FlightDate` | Date of flight departure | Date | Original scheduled date |
| `Reporting_Airline` | IATA code of reporting airline | String | 2-3 letter code |
| `Marketing_Airline_Network` | Airline name | String | Full airline name |
| `Origin` | Origin airport IATA code | String | 3-letter code |
| `Dest` | Intended destination IATA code | String | 3-letter code |
| `DepTime` | Scheduled departure time | Integer | HHMM format |
| `ActualDepTime` | Actual departure time | Integer | HHMM format |
| `DepDelay` | Departure delay in minutes | Float | Negative = early, Positive = late |
| `Div1Airport` | First diversion airport | String | If diverted |
| `Duplicate` | Record duplicate flag | String | Y/N |

### Example Data Row

```
FlightDate: 2023-07-15
Marketing_Airline_Network: American Airlines
Origin: LAX
Dest: JFK
Div1Airport: ORD
DepDelay: 45
DepTime: 1430
ActualDepTime: 1515
```

---

## Data Cleaning Pipeline

### Step 1: File Consolidation

Multiple monthly CSV files were combined into a single dataset:

```python
import pandas as pd
import glob

# List all flight data files (2021-07 to 2024-12)
file_list = sorted(glob.glob('data/*_OTMC_OTP.csv')) 

# Verify all files have consistent column names
columns_set = []
for file in file_list:
    df = pd.read_csv(file, nrows=0)  
    columns_set.append(set(df.columns))

# Combine files
flights_list = [pd.read_csv(f) for f in file_list]
flights = pd.concat(flights_list, ignore_index=True)
```

**Result**: 42 monthly files successfully consolidated into single DataFrame
**Total Records**: 25,386,632 flight records

---

### Step 2: Diversion Identification

Not all flights in the dataset represent diversions. A flight is considered "diverted" when:

1. The flight has a recorded `Div1Airport` value
2. The `Div1Airport` is **different** from the `Dest` (intended destination)
3. The flight was not marked as duplicate

```python
# Filter diverted flights only
diverted_flights = flights[
    (flights['Div1Airport'].notna() & 
     (flights['Div1Airport'] != flights['Dest']))
].copy()

# Remove duplicates
diverted_flights = diverted_flights.drop(
    diverted_flights[diverted_flights['Duplicate'] == 'Y'].index
)
```

**Result**: [Y] diverted flights identified
**Filtering Ratio**: ~[Z]% of all flights were diverted

---

### Step 3: Missing Data Handling

**Empty Columns Removed**: 
- Columns with 100% missing values were dropped
- Reduced dimensionality for cleaner analysis

**Time Data Cleaning**:
- `FlightDate` converted to datetime format
- Missing departure times handled (indicates cancelled flights)
- Departure delays filled with 0 where missing

```python
# Drop empty columns
empty_columns = diverted_flights.columns[diverted_flights.isna().all()]
diverted_flights = diverted_flights.dropna(axis=1, how='all')

# Handle time data
diverted_flights['FlightDate'] = pd.to_datetime(
    diverted_flights['FlightDate'], 
    errors='coerce'
)
```

---

### Step 4: Feature Engineering

**Actual Departure DateTime**

The scheduled departure time was converted to actual departure time using:

```python
diverted_flights['ActualDep'] = (
    diverted_flights['FlightDate'] +
    pd.to_timedelta(diverted_flights['DepTime'] // 100, unit='h') +
    pd.to_timedelta(diverted_flights['DepTime'] % 100, unit='m') +
    pd.to_timedelta(diverted_flights['DepDelay'], unit='m')
)

diverted_flights['ActualDepDate'] = diverted_flights['ActualDep'].dt.date
diverted_flights['ActualDepTime'] = diverted_flights['ActualDep'].dt.time
```

This engineering captures the true departure date (which may differ from scheduled date when flights depart late the previous day).

---

## Data Quality Assessment

### Data Completeness

- **Total Diversion Records**: 64,816
- **Records with Complete Coordinates**: 100% (after geocoding)
- **Records with Airlines**: 100%

### Data Consistency Checks

✅ **Passed**:
- All airports are valid IATA codes
- Delay values are numeric and reasonable
- Dates are in valid range (2021-07 to 2024-12)
- Carrier codes are consistent with FAA registry

⚠️ **Warnings**:
- Some records have missing airport coordinates (handled by geocoding)

---

## Airport Geocoding

To enable geographic analysis, airports were geocoded with latitude/longitude:

### Airport Coordinates Source

Airport coordinates were obtained from the Federal Aviation Administration and OpenFlights Database:
- IATA airport codes matched to latitude/longitude coordinates
- 408 U.S. airports successfully geocoded
- Coordinates precise to seconds (approximately ±30 meters)

```python
# Example geocoding
airports_data = pd.read_csv('airport_coordinates.csv')
airports_clean = airports_data.set_index('IATA_Code')[['latitude', 'longitude']]

def get_coords(airport_code, airport_df):
    if airport_code in airport_df.index:
        coords = airport_df.loc[airport_code]
        return (coords['latitude'], coords['longitude'])
    return (None, None)

# Apply to flight data
diverted_flights['origin_lat'] = diverted_flights['Origin'].apply(
    lambda x: get_coords(x, airports_clean)[0]
)
diverted_flights['origin_lon'] = diverted_flights['Origin'].apply(
    lambda x: get_coords(x, airports_clean)[1]
)
```

**Geocoding Success Rate**: 99.75%

---

## Data Limitations

### Known Limitations

1. **U.S. Domestic Only**: Analysis limited to U.S. domestic flights; international diversions excluded
2. **Carrier Reporting**: Data depends on airline reporting accuracy
3. **Weather Data Not Included**: The current dataset does not include concurrent weather conditions
4. **Secondary Diversions**: Only first diversion recorded; subsequent diversions not tracked
5. **Seasonality**: Dataset spans multiple years but seasonal patterns may be confounded with operational changes

### Missing Information

- Weather conditions at time of diversion
- Reason for diversion (weather, mechanical, ATC, etc.)
- Passenger impact (number of passengers affected)
- Financial impact to airlines

---

## Data Processing Summary

| Stage | Records In | Records Out | % Retained | Notes |
|-------|-----------|------------|-----------|-------|
| Raw Data | [X] | - | - | Consolidated from 45 monthly files |
| Filter Diversions | [X] | [Y] | [Z]% | Removed non-diverted flights |
| Remove Duplicates | [Y] | [Y2] | [Z2]% | BTS-marked duplicates removed |
| Geocoding | [Y2] | [Y3] | [Z3]% | Airports successfully geocoded |
| **Final Dataset** | - | **[Y3]** | **[%]** | Ready for analysis |

---

## Reproducibility

To reproduce this data collection process:

1. Download BTS data from: https://www.transtats.bts.gov/
2. Place monthly CSV files in `/data` directory
3. Run data cleaning script: `notebooks/01-data-cleaning.ipynb`
4. Verify output: `diverted_flights.csv` should contain [Y3] records

### Environment

```
python: 3.13.5
pandas: [VERSION]
geopandas: [VERSION]
numpy: [VERSION]
```

---

## Next Steps

After collection and cleaning, the data was ready for:

1. **Exploratory Analysis** - Understanding distribution and patterns
2. **Temporal Analysis** - Identifying clusters and trends over time
3. **Geospatial Analysis** - Mapping diversions and routes
4. **Clustering** - Finding patterns in diversion events

See the [Analysis](02-analysis.qmd) section for details on methodology.

---

## Data References

- Bureau of Transportation Statistics (BTS): https://www.transtats.bts.gov/
- FAA Airport Data: https://www.faa.gov/
- [Any other sources cited]

---

**Data Collection Date**: [DATE]
**Last Updated**: [DATE]
